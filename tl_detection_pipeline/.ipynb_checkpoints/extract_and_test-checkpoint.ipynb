{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Light Detection and Classification - Udacity CarND Capstone\n",
    "Using a pre-trained model to detect objects in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# Used for image processing and manipulations\n",
    "import cv2\n",
    "\n",
    "# Used for internal storage\n",
    "import pickle\n",
    "\n",
    "# Used for data augmentation\n",
    "from skimage import exposure\n",
    "from skimage import util\n",
    "\n",
    "# Used for dividing the data into training and testing\n",
    "import pandas as pd\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ROOT = r'C:/Users/saidz/git/CarND-Capstone/tl_detection_pipeline/'\n",
    "PATH_TO_OUTPUT = r'C:/Users/saidz/git/CarND-Capstone/tl_detection_pipeline/trained_output/'\n",
    "PATH_TO_API = r'C:/Users/saidz/tensorflow/models/research'\n",
    "\n",
    "sys.path.append(PATH_TO_API)\n",
    "sys.path.append(PATH_TO_API + '/object_detection')\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New frozen file will be generated in pretrained/reextracted_frozen_frcnn.\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_LABELS = r'annotations/label_map.pbtxt'\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "os.chdir(PATH_TO_ROOT)\n",
    "\n",
    "M = \"INCEPTION\" # Fast labeling, but misses\n",
    "M = \"FRCNN\" # More accurate labeling\n",
    "\n",
    "if (M == \"FRCNN\"):\n",
    "    try:\n",
    "        shutil.rmtree('annotation_pipeline/pretrained/reextracted_frozen_frcnn')\n",
    "    except OSError as e:\n",
    "        print (\"New frozen file will be generated in pretrained/reextracted_frozen_frcnn.\")\n",
    "\n",
    "    !python C:/Users/saidz/tensorflow/models/research/object_detection/export_inference_graph.py \\\n",
    "        --pipeline_config_path=pretrained/train_config/faster_rcnn_resnet101_coco.config \\\n",
    "        --trained_checkpoint_prefix=training/model.ckpt \\\n",
    "        --output_directory=trained_output/final_frozen_frcnn/\n",
    "    PATH_TO_MODEL = PATH_TO_ROOT + r'trained_output/final_frozen_frcnn/frozen_inference_graph.pb'\n",
    "elif (M == \"INCEPTION\"):\n",
    "    try:\n",
    "        shutil.rmtree('annotation_pipeline/pretrained/reextracted_frozen_inception')\n",
    "    except OSError as e:\n",
    "        print (\"New frozen file will be generated in pretrained/reextracted_frozen_inception.\")\n",
    "        \n",
    "    !python c:/users/saidz/tensorflow/models/research/object_detection/export_inference_graph.py \\\n",
    "        --pipeline_config_path=pretrained/annotation_config/ssd_inception_v2_coco.config \\\n",
    "        --trained_checkpoint_prefix=pretrained/ssd_inception_v2_coco_2018_01_28/model.ckpt \\\n",
    "        --output_directory=pretrained/reextracted_frozen_inception/\n",
    "    PATH_TO_MODEL = PATH_TO_ROOT + r'pretrained/reextracted_frozen_inception/frozen_inference_graph.pb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python C:/Users/saidz/tensorflow/models/research/object_detection/export_inference_graph.py --pipeline_config_path=pretrained/train_config/faster_rcnn_resnet101_coco.config --trained_checkpoint_prefix=training/model.ckpt-614 --output_directory=trained_output/final_frozen_frcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = PATH_TO_ROOT + r'trained_output/final_frozen_frcnn/frozen_inference_graph.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_MODEL, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"detection_model.image_tensor: \")\n",
    "print(detection_graph.get_tensor_by_name('image_tensor:0'), \"\\n\")\n",
    "print(\"detection_model.detection_boxes: \")\n",
    "print(detection_graph.get_tensor_by_name('detection_boxes:0'), \"\\n\")\n",
    "print(\"detection_model.detection_scores: \")\n",
    "print(detection_graph.get_tensor_by_name('detection_scores:0'), \"\\n\")\n",
    "print(\"detection_model.detection_classes: \")\n",
    "print(detection_graph.get_tensor_by_name('detection_classes:0'), \"\\n\")\n",
    "print(\"detection_model.num_detections: \")\n",
    "print(detection_graph.get_tensor_by_name('num_detections:0'), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = 'C:/Users/saidz/git/TrafficLight_Detection-TensorFlowAPI/test_images_sim/'\n",
    "PATH_TO_TEST_IMAGES_DIR = 'annotation_pipeline/raw-images-simulation/'\n",
    "\n",
    "print(os.path.join(PATH_TO_TEST_IMAGES_DIR, '*/*.jpg'))\n",
    "TEST_IMAGE_PATHS = glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, '*/*.jpg'))\n",
    "print(\"Length of test images:\", len(TEST_IMAGE_PATHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict =[]\n",
    "rows = len(TEST_IMAGE_PATHS) // 5 + 1\n",
    "fig = plt.figure(figsize=(12,3*rows))\n",
    "\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        \n",
    "        # Each box represents a part of the image where a particular object was detected.\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        \n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        \n",
    "        for icount in range(len(TEST_IMAGE_PATHS)):\n",
    "            print('Number {} out of {}, file name: {}'.format(icount, len(TEST_IMAGE_PATHS), TEST_IMAGE_PATHS[icount]))\n",
    "            image_path = TEST_IMAGE_PATHS[icount]\n",
    "            image = Image.open(image_path)\n",
    "\n",
    "            # the array based representation of the image will be used later in order to prepare the\n",
    "            # result image with boxes and labels on it.\n",
    "            image_np = load_image_into_numpy_array(image)\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            time0 = time.time()\n",
    "\n",
    "            # Actual detection.\n",
    "            (boxes, scores, classes, num) = sess.run(\n",
    "              [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "              feed_dict={image_tensor: image_np_expanded})\n",
    "\n",
    "            time1 = time.time()\n",
    "\n",
    "            boxes = np.squeeze(boxes)\n",
    "            scores = np.squeeze(scores)\n",
    "            classes = np.squeeze(classes).astype(np.int32)\n",
    "            \n",
    "            color = str(TEST_IMAGE_PATHS[icount]).replace('\\\\','/').split('/')[-2]\n",
    "\n",
    "            output_dict.append({'path': str(TEST_IMAGE_PATHS[icount]).replace('\\\\','/'), 'color': color, \\\n",
    "                                'boxes': boxes, 'scores': scores, 'classes': classes})\n",
    "            \n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np, boxes, classes, scores,\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=6)\n",
    "\n",
    "            ax = plt.subplot(rows,5,len(output_dict))\n",
    "            ax.imshow(image_np)\n",
    "            ax.set_title(\"{}: {}\".format(color, len(output_dict[icount]['boxes'])), fontsize=10)\n",
    "            ax.axis(\"off\")\n",
    "                    \n",
    "plt.show()\n",
    "pickle.dump(output_dict, open(PATH_TO_ROOT + '/annotation_pipeline/' + 'annotations' + '.p', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations(annotation, image_path, show_image=False):\n",
    "    output={}\n",
    "    output[\"file_name\"] = annotation[\"path\"]\n",
    "    output[\"color\"] = annotation[\"color\"]\n",
    "    output[\"boxes\"] =[]\n",
    "    output[\"flipped_boxes\"] =[]\n",
    "    image = plt.imread(output[\"file_name\"])\n",
    "#     image = cv2.resize(image,(300, 300))\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "\n",
    "    y_scale = image.shape[0]\n",
    "    x_scale = image.shape[1]\n",
    "    print(image.shape)\n",
    "\n",
    "    boxed_image = image\n",
    "    boxed_flipped_image = flipped_image\n",
    "    print(len(annotation[\"boxes\"]))\n",
    "    for i in range(len(annotation[\"boxes\"])):\n",
    "        box = annotation[\"boxes\"][i]\n",
    "        start_point = (int(x_scale*box[1]), int(y_scale*box[0]))\n",
    "        end_point = (int(x_scale*box[3]), int(y_scale*box[2]))\n",
    "        flipped_start_point = (int(x_scale*(1-box[1])), int(y_scale*box[0]))\n",
    "        flipped_end_point = (int(x_scale*(1-box[3])), int(y_scale*box[2]))\n",
    "        height = end_point[1] - start_point[1]\n",
    "        width  = end_point[0] - start_point[0]\n",
    "        img = image[start_point[1]:end_point[1], start_point[0]:end_point[0], :]\n",
    "        imshape = img.shape\n",
    "        imsize = imshape[0] * imshape[1]\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # color range\n",
    "        lower_red1 = np.array([0,100,100])\n",
    "        upper_red1 = np.array([10,255,255])\n",
    "        lower_red2 = np.array([160,100,100])\n",
    "        upper_red2 = np.array([180,255,255])\n",
    "        lower_green = np.array([40,50,50])\n",
    "        upper_green = np.array([90,255,255])\n",
    "        # lower_yellow = np.array([15,100,100])\n",
    "        # upper_yellow = np.array([35,255,255])\n",
    "        lower_yellow = np.array([15,150,150])\n",
    "        upper_yellow = np.array([35,255,255])\n",
    "        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "        maskg = cv2.inRange(hsv, lower_green, upper_green)\n",
    "        masky = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        maskr = cv2.add(mask1, mask2)\n",
    "        \n",
    "        #print(\"Red: {}, Green: {}, Yellow: {}\".format(maskr.shape, maskg.shape, masky.shape))\n",
    "        #print(\"Size: {}, Red: {}, Green: {}, Yellow: {}\".format(imsize, (maskr>0).sum(), (maskg>0).sum(), (masky>0).sum()))\n",
    "        \n",
    "        r_size = (maskr>0).sum() # ((maskr>0) * (maskg==0) * (masky == 0)).sum()\n",
    "        g_size = (maskg>0).sum()\n",
    "        y_size = (masky>0).sum()\n",
    "        \n",
    "        most_color = \"\"\n",
    "        if (r_size > max(imsize/50, g_size, y_size)):\n",
    "            most_color = 'red'\n",
    "        elif (g_size > max(imsize/50, r_size, y_size)):\n",
    "            most_color = 'green'\n",
    "        elif (y_size > max(imsize/50, g_size, r_size)):\n",
    "            most_color = 'yellow'\n",
    "\n",
    "        aspect_ratio = height/width\n",
    "        x_ratio = width / x_scale\n",
    "        y_ratio = height / y_scale\n",
    "        if (aspect_ratio > 1.5) and (aspect_ratio < 10) and (annotation['scores'][i] > 0.2) and \\\n",
    "            (x_ratio < 1/6) and (y_ratio < 1/2) :\n",
    "            if ((most_color == annotation[\"color\"]) or (len(annotation[\"boxes\"]) == 1)):\n",
    "                #print(\"Aspect ratio: {}, x_ratio: {}, y_ratio: {}, Score: {}\".format(aspect_ratio, x_ratio, y_ratio, annotation['detection_scores'][i]))\n",
    "        \n",
    "                output[\"boxes\"].append([start_point,end_point])\n",
    "                output[\"flipped_boxes\"].append([flipped_start_point,flipped_end_point])\n",
    "                boxed_image = cv2.rectangle(boxed_image,start_point,end_point,(255,255,255),2)\n",
    "                boxed_flipped_image = cv2.rectangle(boxed_flipped_image,flipped_start_point,flipped_end_point,(255,255,255),2)\n",
    "    if (show_image):\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        ax = plt.subplot(1,2,1)\n",
    "        ax.imshow(boxed_image)   \n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1,2,2)\n",
    "        ax.imshow(boxed_flipped_image)\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "        print(output[\"boxes\"])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_annotations = pickle.load(open(PATH_TO_ROOT + '/annotation_pipeline/' + 'annotations' + '.p', 'rb'))\n",
    "filtered_annotations = []\n",
    "for i in range(0,len(read_annotations)):\n",
    "    filtered_annotations.append(filter_annotations(read_annotations[i], str(PATH_TO_TEST_IMAGES_DIR),show_image=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_brightness(image):    \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    image_HLS = np.array(image_HLS, dtype = np.float64)     \n",
    "    random_brightness_coefficient = np.random.uniform()+0.5 ## generates value between 0.5 and 1.5    \n",
    "    brightness_coefficient = 1.2\n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1]*brightness_coefficient ## scale pixel values up or down for channel 1(Lightness)    \n",
    "    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255    \n",
    "    image_HLS = np.array(image_HLS, dtype = np.uint8)    \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def generate_random_lines(imshape,slant,drop_length):    \n",
    "    drops=[]    \n",
    "    for i in range(1500): ## If You want heavy rain, try increasing this        \n",
    "        if slant<0:            \n",
    "            x= np.random.randint(slant,imshape[1])        \n",
    "        else:            \n",
    "            x= np.random.randint(0,imshape[1]-slant)        \n",
    "        y= np.random.randint(0,imshape[0]-drop_length)        \n",
    "        drops.append((x,y))    \n",
    "    return drops            \n",
    "\n",
    "def add_rain(image):        \n",
    "    image = image.copy()\n",
    "    imshape = image.shape    \n",
    "    slant_extreme=10    \n",
    "    slant= np.random.randint(-slant_extreme,slant_extreme)     \n",
    "    drop_length=20    \n",
    "    drop_width=2 \n",
    "    drop_color=(200,200,200) \n",
    "    ## a shade of gray    \n",
    "    rain_drops= generate_random_lines(imshape,slant,drop_length)        \n",
    "    for rain_drop in rain_drops:        \n",
    "        cv2.line(image,(rain_drop[0],rain_drop[1]),(rain_drop[0]+slant,rain_drop[1]+drop_length),drop_color,drop_width)    \n",
    "    image= cv2.blur(image,(7,7)) ## rainy view are blurry        \n",
    "    brightness_coefficient = 0.7 ## rainy days are usually shady     \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1]*brightness_coefficient ## scale pixel values down for channel 1(Lightness)    \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def generate_shadow_coordinates(imshape, no_of_shadows=1):    \n",
    "    vertices_list=[]    \n",
    "    for index in range(no_of_shadows):        \n",
    "        vertex=[]        \n",
    "        for dimensions in range(np.random.randint(3,15)): ## Dimensionality of the shadow polygon            \n",
    "            vertex.append(( imshape[1]*np.random.uniform(),imshape[0]//3+imshape[0]*np.random.uniform()))        \n",
    "            vertices = np.array([vertex], dtype=np.int32) ## single shadow vertices         \n",
    "            vertices_list.append(vertices)\n",
    "    return vertices_list ## List of shadow vertices\n",
    "\n",
    "\n",
    "def add_shadow(image,no_of_shadows=1):    \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    mask = np.zeros_like(image)     \n",
    "    imshape = image.shape    \n",
    "    vertices_list= generate_shadow_coordinates(imshape, no_of_shadows) #3 getting list of shadow vertices    \n",
    "    for vertices in vertices_list:         \n",
    "        cv2.fillPoly(mask, vertices, 255) ## adding all shadow polygons on empty mask, single 255 denotes only red channel        \n",
    "    image_HLS[:,:,1][mask[:,:,0]==255] = image_HLS[:,:,1][mask[:,:,0]==255]*7   ## if red channel is hot, image's \"Lightness\" channel's brightness is lowered     \n",
    "    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255 \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def augmentData(original_image):\n",
    "    \n",
    "    blured_image = cv2.GaussianBlur(original_image,(15,15),0)\n",
    "    image_with_random_noise = np.uint8(util.random_noise(original_image)*255)\n",
    "    image_with_random_noise[image_with_random_noise > 255]  = 255\n",
    "    image_with_random_noise[image_with_random_noise < 0]  = 0\n",
    "    image_with_random_noise = np.uint8(image_with_random_noise)\n",
    "    v_min, v_max = np.percentile(original_image, (0.2, 99.8))\n",
    "    better_contrast = exposure.rescale_intensity(original_image, in_range=(v_min, v_max))\n",
    "    bright_image = add_brightness(original_image)\n",
    "    rainy_image = add_rain(original_image)\n",
    "    shadowed_image = add_shadow(original_image,no_of_shadows=1)\n",
    "    \n",
    "    return [original_image, blured_image, image_with_random_noise, bright_image, rainy_image]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_list(probability, choice, alternative_choice):\n",
    "    if (random() <= probability):\n",
    "        return choice\n",
    "    else:\n",
    "        return alternative_choice\n",
    "\n",
    "def make_cvs_file(annotations, output_path, test_partition = 0.15):\n",
    "    \n",
    "    print(\"---> \", output_path)\n",
    "    annotation_lists = {}\n",
    "    annotation_lists[\"train\"] = []\n",
    "    annotation_lists[\"test\"] = []\n",
    "    counter = 0\n",
    "    for i in range(len(annotations)):\n",
    "        counter += 1\n",
    "        fname = str(counter).zfill(5)\n",
    "        \n",
    "        image = plt.imread(annotations[i][\"file_name\"])\n",
    "        \n",
    "        width = image.shape[1]\n",
    "        height = image.shape[0]\n",
    "        class_name = annotations[i][\"color\"]\n",
    "        class_no   = 0\n",
    "        if (class_name == \"red\"):\n",
    "            class_no = 1\n",
    "        elif (class_name == \"yellow\"):\n",
    "            class_no = 2\n",
    "        elif (class_name == \"green\"):\n",
    "            class_no = 3\n",
    "        \n",
    "        boxes = annotations[i][\"boxes\"]\n",
    "        flipped_boxes = annotations[i][\"flipped_boxes\"]\n",
    "        \n",
    "        if (len(boxes) > 0):\n",
    "            images = augmentData(image)\n",
    "            \n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_rigal\" + \".jpg\"\n",
    "            plt.imsave(filename, images[0], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_blure\" + \".jpg\"\n",
    "            plt.imsave(filename, images[1], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_noise\" + \".jpg\"\n",
    "            plt.imsave(filename, images[2], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_brght\" + \".jpg\"\n",
    "            plt.imsave(filename, images[3], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_rainy\" + \".jpg\"\n",
    "            plt.imsave(filename, images[4], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "\n",
    "            images = augmentData(cv2.flip(image, 1))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_lippd\" + \".jpg\"\n",
    "            plt.imsave(filename, images[0], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_blure\" + \".jpg\"\n",
    "            plt.imsave(filename, images[1], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_noise\" + \".jpg\"\n",
    "            plt.imsave(filename, images[2], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_brght\" + \".jpg\"\n",
    "            plt.imsave(filename, images[3], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_rainy\" + \".jpg\"\n",
    "            plt.imsave(filename, images[4], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, class_no, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "\n",
    "    \n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                'class_name', 'class_no', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    training_data_frame = pd.DataFrame(annotation_lists[\"train\"], columns=column_name)\n",
    "    training_data_frame.to_csv(output_path + \"/train_annotation_list.csv\")\n",
    "    test_data_frame = pd.DataFrame(annotation_lists[\"test\"], columns=column_name)\n",
    "    test_data_frame.to_csv(output_path + \"/test_annotation_list.csv\")\n",
    "    return test_data_frame, training_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, training_df = make_cvs_file(filtered_annotations, PATH_TO_ROOT + 'annotation_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./annotation_pipeline/generate_tfrecord.py --csv_input=./annotation_pipeline/train_annotation_list.csv  --output_path=./annotations/train.record\n",
    "!python ./annotation_pipeline/generate_tfrecord.py --csv_input=./annotation_pipeline/test_annotation_list.csv   --output_path=./annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python c:/users/saidz/tensorflow/models/research/object_detection/train.py --pipeline_config_path=./pretrained/train_config/faster_rcnn-traffic-udacity_sim.config --train_dir=./training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
