{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "This notebook is based on the tutorial in Object Detection API and should run with Tensorflow 2.0, assuming that the Tensorflow Object Detection API is installed.\n",
    "\n",
    "This notebook assumes \n",
    "1. that images with trafic lights are stored in a directory called 'raw_images_simultion' organized into 4 different subdirectories wccording to the colors of the lights:\n",
    "\n",
    "    raw_images_simulation$ ls\n",
    "    \n",
    "    green  none  red  yellow\n",
    "\n",
    "and does the following:\n",
    "1. Uses a pretrained model and find boxes with potential trafic lights. As the images are already ordered in sub directories, the valid color of the light is known.\n",
    "\n",
    "2. Goes threough all images and boxes and checks that the found box has a reasonable score for being a traffic light, has a reasonable size and aspect ratio and actually contains a trafic light or correct color. In that case the image and the box are taken and are labeled.\n",
    "\n",
    "Please start by setting these two path variables so that the remaining parts work properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set these two paths\n",
    "path_to_tensorflow_models = '/home/said/ML_tools/tensorflow37/models'\n",
    "path_to_tl_detection = '/home/said/GIT/CarND-Capstone/tl_detection_pipeline'\n",
    "\n",
    "import os\n",
    "# We run the code from that directory:\n",
    "os.chdir(path_to_tl_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import tkinter\n",
    "from matplotlib import pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Used for image processing and manipulations\n",
    "import cv2\n",
    "\n",
    "# Used for internal storage\n",
    "import pickle\n",
    "\n",
    "# Used for data augmentation\n",
    "from skimage import exposure\n",
    "from skimage import util\n",
    "\n",
    "# Used for dividing the data into training and testing\n",
    "import pandas as pd\n",
    "from random import seed\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "Import the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-IMl4b6BdGO"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYPCiag2iz_q"
   },
   "source": [
    "Patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mF-YlMl8c_bM"
   },
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables\n",
    "\n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing the path.\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zm8xp-0eoItE"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "  base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  model_file = model_name + '.tar.gz'\n",
    "  model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=base_url + model_file,\n",
    "    untar=True)\n",
    "\n",
    "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "  print(model_dir)\n",
    "\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "  model = model.signatures['serving_default']\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## Loading the model and label map for initial detection\n",
    "The object initial detection model is assumed to exist and will be loaded here. \n",
    "\n",
    "First, these two paths must be set properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/said/.keras/datasets/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/saved_model\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_LABELS = path_to_tensorflow_models + '/research/object_detection/data/mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "model_name = 'ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03'\n",
    "detection_model = load_model(model_name)\n",
    "\n",
    "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('./raw-images-simulation')\n",
    "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*/*.jpg\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yN1AYfAEJIGp"
   },
   "source": [
    "Check the model's input signature, it expects a batch of 3-color images of type uint8: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CK4cnry6wsHY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detection_model.inputs: \n",
      "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>] \n",
      "\n",
      "detection_model.output_dtypes: \n",
      "{'detection_classes': tf.float32, 'num_detections': tf.float32, 'detection_boxes': tf.float32, 'detection_scores': tf.float32} \n",
      "\n",
      "detection_model.output_shapes: \n",
      "{'detection_classes': TensorShape([None, 100]), 'num_detections': TensorShape([None]), 'detection_boxes': TensorShape([None, 100, 4]), 'detection_scores': TensorShape([None, 100])} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"detection_model.inputs: \")\n",
    "print(detection_model.inputs, \"\\n\")\n",
    "print(\"detection_model.output_dtypes: \")\n",
    "print(detection_model.output_dtypes, \"\\n\")\n",
    "print(\"detection_model.output_shapes: \")\n",
    "print(detection_model.output_shapes, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JP5qZ7sXJpwG"
   },
   "source": [
    "Add a wrapper function to call the model, and cleanup the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ajmR_exWyN76"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # Run inference\n",
    "  output_dict = model(input_tensor)\n",
    "\n",
    "  # All outputs are batches tensors.\n",
    "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "  # We're only interested in the first num_detections.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes should be ints.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # Handle models with masks:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # Reframe the the bbox mask to the image size.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                       tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "    \n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1wq0LVyMRR_"
   },
   "source": [
    "Run it on each test image and show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWh_1zz6aqxs"
   },
   "outputs": [],
   "source": [
    "def show_inference(model, image_path):\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "  output_dict['file_name'] = str(image_path).split('/')[-1]\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "\n",
    "  # display(Image.fromarray(image_np))\n",
    "  return output_dict, Image.fromarray(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step of annotations\n",
    "Go through all images and make a list of annotation objects. Save the list as a pickle file so that it can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3a5wMHN8WKMh",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 1 out of 131, file name: raw-images-simulation/green/left0540.jpg\n",
      "Number 2 out of 131, file name: raw-images-simulation/green/left0541.jpg\n",
      "Number 3 out of 131, file name: raw-images-simulation/green/left0542.jpg\n",
      "Number 4 out of 131, file name: raw-images-simulation/green/left0543.jpg\n",
      "Number 5 out of 131, file name: raw-images-simulation/green/left0544.jpg\n",
      "Number 6 out of 131, file name: raw-images-simulation/green/left0545.jpg\n",
      "Number 7 out of 131, file name: raw-images-simulation/green/left0546.jpg\n",
      "Number 8 out of 131, file name: raw-images-simulation/green/left0547.jpg\n",
      "Number 9 out of 131, file name: raw-images-simulation/green/left0548.jpg\n",
      "Number 10 out of 131, file name: raw-images-simulation/green/left0549.jpg\n",
      "Number 11 out of 131, file name: raw-images-simulation/green/left0550.jpg\n",
      "Number 12 out of 131, file name: raw-images-simulation/green/left0551.jpg\n",
      "Number 13 out of 131, file name: raw-images-simulation/green/left0552.jpg\n",
      "Number 14 out of 131, file name: raw-images-simulation/green/left0553.jpg\n",
      "Number 15 out of 131, file name: raw-images-simulation/green/left0554.jpg\n",
      "Number 16 out of 131, file name: raw-images-simulation/green/left0555.jpg\n",
      "Number 17 out of 131, file name: raw-images-simulation/green/left0556.jpg\n",
      "Number 18 out of 131, file name: raw-images-simulation/green/left0557.jpg\n",
      "Number 19 out of 131, file name: raw-images-simulation/green/left0558.jpg\n",
      "Number 20 out of 131, file name: raw-images-simulation/green/left0559.jpg\n",
      "Number 21 out of 131, file name: raw-images-simulation/none/left0040.jpg\n",
      "Number 22 out of 131, file name: raw-images-simulation/none/left0041.jpg\n",
      "Number 23 out of 131, file name: raw-images-simulation/none/left0044.jpg\n",
      "Number 24 out of 131, file name: raw-images-simulation/none/left0045.jpg\n",
      "Number 25 out of 131, file name: raw-images-simulation/none/left0046.jpg\n",
      "Number 26 out of 131, file name: raw-images-simulation/none/left0047.jpg\n",
      "Number 27 out of 131, file name: raw-images-simulation/none/left0049.jpg\n",
      "Number 28 out of 131, file name: raw-images-simulation/none/left0050.jpg\n",
      "Number 29 out of 131, file name: raw-images-simulation/none/left0051.jpg\n",
      "Number 30 out of 131, file name: raw-images-simulation/none/left0054.jpg\n",
      "Number 31 out of 131, file name: raw-images-simulation/none/left0055.jpg\n",
      "Number 32 out of 131, file name: raw-images-simulation/none/left0056.jpg\n",
      "Number 33 out of 131, file name: raw-images-simulation/none/left0057.jpg\n",
      "Number 34 out of 131, file name: raw-images-simulation/none/left0059.jpg\n",
      "Number 35 out of 131, file name: raw-images-simulation/none/left0411.jpg\n",
      "Number 36 out of 131, file name: raw-images-simulation/none/left0414.jpg\n",
      "Number 37 out of 131, file name: raw-images-simulation/none/left0415.jpg\n",
      "Number 38 out of 131, file name: raw-images-simulation/none/left0416.jpg\n",
      "Number 39 out of 131, file name: raw-images-simulation/none/left0417.jpg\n",
      "Number 40 out of 131, file name: raw-images-simulation/none/left0419.jpg\n",
      "Number 41 out of 131, file name: raw-images-simulation/none/left0420.jpg\n",
      "Number 42 out of 131, file name: raw-images-simulation/none/left0421.jpg\n",
      "Number 43 out of 131, file name: raw-images-simulation/none/left0424.jpg\n",
      "Number 44 out of 131, file name: raw-images-simulation/none/left0425.jpg\n",
      "Number 45 out of 131, file name: raw-images-simulation/none/left0426.jpg\n",
      "Number 46 out of 131, file name: raw-images-simulation/red/left0428.jpg\n",
      "Number 47 out of 131, file name: raw-images-simulation/red/left0431.jpg\n",
      "Number 48 out of 131, file name: raw-images-simulation/red/left0432.jpg\n",
      "Number 49 out of 131, file name: raw-images-simulation/red/left0434.jpg\n",
      "Number 50 out of 131, file name: raw-images-simulation/red/left0435.jpg\n",
      "Number 51 out of 131, file name: raw-images-simulation/red/left0437.jpg\n",
      "Number 52 out of 131, file name: raw-images-simulation/red/left0438.jpg\n",
      "Number 53 out of 131, file name: raw-images-simulation/red/left0441.jpg\n",
      "Number 54 out of 131, file name: raw-images-simulation/red/left0442.jpg\n",
      "Number 55 out of 131, file name: raw-images-simulation/red/left0444.jpg\n",
      "Number 56 out of 131, file name: raw-images-simulation/red/left0445.jpg\n",
      "Number 57 out of 131, file name: raw-images-simulation/red/left0447.jpg\n",
      "Number 58 out of 131, file name: raw-images-simulation/red/left0448.jpg\n",
      "Number 59 out of 131, file name: raw-images-simulation/red/left0451.jpg\n",
      "Number 60 out of 131, file name: raw-images-simulation/red/left0452.jpg\n",
      "Number 61 out of 131, file name: raw-images-simulation/red/left0454.jpg\n",
      "Number 62 out of 131, file name: raw-images-simulation/red/left0455.jpg\n",
      "Number 63 out of 131, file name: raw-images-simulation/red/left0457.jpg\n",
      "Number 64 out of 131, file name: raw-images-simulation/red/left0458.jpg\n",
      "Number 65 out of 131, file name: raw-images-simulation/red/left0461.jpg\n",
      "Number 66 out of 131, file name: raw-images-simulation/red/left0462.jpg\n",
      "Number 67 out of 131, file name: raw-images-simulation/red/left0464.jpg\n",
      "Number 68 out of 131, file name: raw-images-simulation/red/left0465.jpg\n",
      "Number 69 out of 131, file name: raw-images-simulation/red/left0467.jpg\n",
      "Number 70 out of 131, file name: raw-images-simulation/red/left0468.jpg\n",
      "Number 71 out of 131, file name: raw-images-simulation/red/left0471.jpg\n",
      "Number 72 out of 131, file name: raw-images-simulation/red/left0472.jpg\n",
      "Number 73 out of 131, file name: raw-images-simulation/red/left0474.jpg\n",
      "Number 74 out of 131, file name: raw-images-simulation/red/left0475.jpg\n",
      "Number 75 out of 131, file name: raw-images-simulation/red/left0477.jpg\n",
      "Number 76 out of 131, file name: raw-images-simulation/red/left0478.jpg\n",
      "Number 77 out of 131, file name: raw-images-simulation/red/left0491.jpg\n",
      "Number 78 out of 131, file name: raw-images-simulation/red/left0492.jpg\n",
      "Number 79 out of 131, file name: raw-images-simulation/red/left0494.jpg\n",
      "Number 80 out of 131, file name: raw-images-simulation/red/left0495.jpg\n",
      "Number 81 out of 131, file name: raw-images-simulation/red/left0497.jpg\n",
      "Number 82 out of 131, file name: raw-images-simulation/red/left0498.jpg\n",
      "Number 83 out of 131, file name: raw-images-simulation/red/left0601.jpg\n",
      "Number 84 out of 131, file name: raw-images-simulation/red/left0602.jpg\n",
      "Number 85 out of 131, file name: raw-images-simulation/red/left0604.jpg\n",
      "Number 86 out of 131, file name: raw-images-simulation/red/left0605.jpg\n",
      "Number 87 out of 131, file name: raw-images-simulation/red/left0607.jpg\n",
      "Number 88 out of 131, file name: raw-images-simulation/red/left0608.jpg\n",
      "Number 89 out of 131, file name: raw-images-simulation/red/left0611.jpg\n",
      "Number 90 out of 131, file name: raw-images-simulation/red/left0612.jpg\n",
      "Number 91 out of 131, file name: raw-images-simulation/red/left0614.jpg\n",
      "Number 92 out of 131, file name: raw-images-simulation/red/left0615.jpg\n",
      "Number 93 out of 131, file name: raw-images-simulation/red/left0617.jpg\n",
      "Number 94 out of 131, file name: raw-images-simulation/red/left0618.jpg\n",
      "Number 95 out of 131, file name: raw-images-simulation/red/left0621.jpg\n",
      "Number 96 out of 131, file name: raw-images-simulation/red/left0622.jpg\n",
      "Number 97 out of 131, file name: raw-images-simulation/red/left0624.jpg\n",
      "Number 98 out of 131, file name: raw-images-simulation/red/left0625.jpg\n",
      "Number 99 out of 131, file name: raw-images-simulation/red/left0937.jpg\n",
      "Number 100 out of 131, file name: raw-images-simulation/red/left0971.jpg\n",
      "Number 101 out of 131, file name: raw-images-simulation/red/left0972.jpg\n",
      "Number 102 out of 131, file name: raw-images-simulation/red/left0984.jpg\n",
      "Number 103 out of 131, file name: raw-images-simulation/red/left0987.jpg\n",
      "Number 104 out of 131, file name: raw-images-simulation/red/left0988.jpg\n",
      "Number 105 out of 131, file name: raw-images-simulation/red/left0991.jpg\n",
      "Number 106 out of 131, file name: raw-images-simulation/yellow/left0011.jpg\n",
      "Number 107 out of 131, file name: raw-images-simulation/yellow/left0012.jpg\n",
      "Number 108 out of 131, file name: raw-images-simulation/yellow/left0014.jpg\n",
      "Number 109 out of 131, file name: raw-images-simulation/yellow/left0015.jpg\n",
      "Number 110 out of 131, file name: raw-images-simulation/yellow/left0016.jpg\n",
      "Number 111 out of 131, file name: raw-images-simulation/yellow/left0017.jpg\n",
      "Number 112 out of 131, file name: raw-images-simulation/yellow/left0019.jpg\n",
      "Number 113 out of 131, file name: raw-images-simulation/yellow/left0020.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number 114 out of 131, file name: raw-images-simulation/yellow/left0021.jpg\n",
      "Number 115 out of 131, file name: raw-images-simulation/yellow/left0022.jpg\n",
      "Number 116 out of 131, file name: raw-images-simulation/yellow/left0570.jpg\n",
      "Number 117 out of 131, file name: raw-images-simulation/yellow/left0571.jpg\n",
      "Number 118 out of 131, file name: raw-images-simulation/yellow/left0572.jpg\n",
      "Number 119 out of 131, file name: raw-images-simulation/yellow/left0574.jpg\n",
      "Number 120 out of 131, file name: raw-images-simulation/yellow/left0575.jpg\n",
      "Number 121 out of 131, file name: raw-images-simulation/yellow/left0576.jpg\n",
      "Number 122 out of 131, file name: raw-images-simulation/yellow/left0577.jpg\n",
      "Number 123 out of 131, file name: raw-images-simulation/yellow/left0579.jpg\n",
      "Number 124 out of 131, file name: raw-images-simulation/yellow/left0580.jpg\n",
      "Number 125 out of 131, file name: raw-images-simulation/yellow/left0581.jpg\n",
      "Number 126 out of 131, file name: raw-images-simulation/yellow/left0582.jpg\n",
      "Number 127 out of 131, file name: raw-images-simulation/yellow/left0584.jpg\n",
      "Number 128 out of 131, file name: raw-images-simulation/yellow/left0585.jpg\n",
      "Number 129 out of 131, file name: raw-images-simulation/yellow/left0586.jpg\n",
      "Number 130 out of 131, file name: raw-images-simulation/yellow/left0587.jpg\n",
      "Number 131 out of 131, file name: raw-images-simulation/yellow/left0589.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "rows = len(TEST_IMAGE_PATHS) // 5 + 1\n",
    "fig = plt.figure(figsize=(12,3*rows))\n",
    "annotations = []\n",
    "counter = 0\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  counter += 1\n",
    "  print('Number {} out of {}, file name: {}'.format(counter, len(TEST_IMAGE_PATHS), image_path))\n",
    "  output_dict, boxed_image = show_inference(detection_model, image_path)\n",
    "  color = str(image_path).split('/')[-2]\n",
    "  output_dict['color'] = color\n",
    "  annotations.append(output_dict)\n",
    "  ax = plt.subplot(rows,5,len(annotations))\n",
    "  ax.imshow(boxed_image)\n",
    "  ax.set_title(\"{}: {}\".format(color, output_dict['num_detections']), fontsize=10)\n",
    "  ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "pickle.dump(annotations, open(path_to_tl_detection + '/annotations/' + model_name + '.p', 'wb')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and augmentation\n",
    "Define the filter function and filter the annotated boxes so that correct boxes are remaining. After filtering, data should be augmented. For that, further image processing steps are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_annotations(annotation, image_path, show_image=False):\n",
    "    #print(annotation)\n",
    "    # [y_min, x_min, y_max, x_max]\n",
    "    output={}\n",
    "    output[\"file_name\"] = image_path + \"/\" + annotation[\"color\"] + \"/\" + annotation[\"file_name\"]\n",
    "    output[\"color\"] = annotation[\"color\"]\n",
    "    output[\"boxes\"] =[]\n",
    "    output[\"flipped_boxes\"] =[]\n",
    "    image = plt.imread(output[\"file_name\"])\n",
    "    flipped_image = cv2.flip(image, 1)\n",
    "\n",
    "    y_scale = image.shape[0]\n",
    "    x_scale = image.shape[1]\n",
    "\n",
    "    boxed_image = image\n",
    "    boxed_flipped_image = flipped_image\n",
    "    for i in range(len(annotation[\"detection_boxes\"])):\n",
    "        box = annotation[\"detection_boxes\"][i]\n",
    "        start_point = (int(x_scale*box[1]), int(y_scale*box[0]))\n",
    "        end_point = (int(x_scale*box[3]), int(y_scale*box[2]))\n",
    "        flipped_start_point = (int(x_scale*(1-box[1])), int(y_scale*box[0]))\n",
    "        flipped_end_point = (int(x_scale*(1-box[3])), int(y_scale*box[2]))\n",
    "        height = end_point[1] - start_point[1]\n",
    "        width  = end_point[0] - start_point[0]\n",
    "        img = image[start_point[1]:end_point[1], start_point[0]:end_point[0], :]\n",
    "        imshape = img.shape\n",
    "        imsize = imshape[0] * imshape[1]\n",
    "        \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # color range\n",
    "        lower_red1 = np.array([0,100,100])\n",
    "        upper_red1 = np.array([10,255,255])\n",
    "        lower_red2 = np.array([160,100,100])\n",
    "        upper_red2 = np.array([180,255,255])\n",
    "        lower_green = np.array([40,50,50])\n",
    "        upper_green = np.array([90,255,255])\n",
    "        # lower_yellow = np.array([15,100,100])\n",
    "        # upper_yellow = np.array([35,255,255])\n",
    "        lower_yellow = np.array([15,150,150])\n",
    "        upper_yellow = np.array([35,255,255])\n",
    "        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n",
    "        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n",
    "        maskg = cv2.inRange(hsv, lower_green, upper_green)\n",
    "        masky = cv2.inRange(hsv, lower_yellow, upper_yellow)\n",
    "        maskr = cv2.add(mask1, mask2)\n",
    "        \n",
    "        #print(\"Red: {}, Green: {}, Yellow: {}\".format(maskr.shape, maskg.shape, masky.shape))\n",
    "        #print(\"Size: {}, Red: {}, Green: {}, Yellow: {}\".format(imsize, (maskr>0).sum(), (maskg>0).sum(), (masky>0).sum()))\n",
    "        \n",
    "        r_size = (maskr>0).sum() # ((maskr>0) * (maskg==0) * (masky == 0)).sum()\n",
    "        g_size = (maskg>0).sum()\n",
    "        y_size = (masky>0).sum()\n",
    "        \n",
    "        most_color = \"\"\n",
    "        if (r_size > max(imsize/50, g_size, y_size)):\n",
    "            most_color = 'red'\n",
    "        elif (g_size > max(imsize/50, r_size, y_size)):\n",
    "            most_color = 'green'\n",
    "        elif (y_size > max(imsize/50, g_size, r_size)):\n",
    "            most_color = 'yellow'\n",
    "\n",
    "        aspect_ratio = height/width\n",
    "        x_ratio = width / x_scale\n",
    "        y_ratio = height / y_scale\n",
    "        if (aspect_ratio > 1.5) and (aspect_ratio < 10) and (annotation['detection_scores'][i] > 0.2) and \\\n",
    "            (x_ratio < 1/6) and (y_ratio < 1/2) :\n",
    "            if ((most_color == annotation[\"color\"]) or (len(annotation[\"detection_boxes\"]) == 1)):\n",
    "                #print(\"Aspect ratio: {}, x_ratio: {}, y_ratio: {}, Score: {}\".format(aspect_ratio, x_ratio, y_ratio, annotation['detection_scores'][i]))\n",
    "        \n",
    "                output[\"boxes\"].append([start_point,end_point])\n",
    "                output[\"flipped_boxes\"].append([flipped_start_point,flipped_end_point])\n",
    "                boxed_image = cv2.rectangle(boxed_image,start_point,end_point,(0,255,255),4)\n",
    "                boxed_flipped_image = cv2.rectangle(boxed_flipped_image,flipped_start_point,flipped_end_point,(0,255,255),4)\n",
    "    if (show_image):\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        ax = plt.subplot(1,2,1)\n",
    "        ax.imshow(boxed_image)   \n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(1,2,2)\n",
    "        ax.imshow(boxed_flipped_image)\n",
    "        ax.axis(\"off\")\n",
    "        plt.show()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_brightness(image):    \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    image_HLS = np.array(image_HLS, dtype = np.float64)     \n",
    "    random_brightness_coefficient = np.random.uniform()+0.5 ## generates value between 0.5 and 1.5    \n",
    "    brightness_coefficient = 1.2\n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1]*brightness_coefficient ## scale pixel values up or down for channel 1(Lightness)    \n",
    "    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255    \n",
    "    image_HLS = np.array(image_HLS, dtype = np.uint8)    \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def generate_random_lines(imshape,slant,drop_length):    \n",
    "    drops=[]    \n",
    "    for i in range(1500): ## If You want heavy rain, try increasing this        \n",
    "        if slant<0:            \n",
    "            x= np.random.randint(slant,imshape[1])        \n",
    "        else:            \n",
    "            x= np.random.randint(0,imshape[1]-slant)        \n",
    "        y= np.random.randint(0,imshape[0]-drop_length)        \n",
    "        drops.append((x,y))    \n",
    "    return drops            \n",
    "\n",
    "def add_rain(image):        \n",
    "    image = image.copy()\n",
    "    imshape = image.shape    \n",
    "    slant_extreme=10    \n",
    "    slant= np.random.randint(-slant_extreme,slant_extreme)     \n",
    "    drop_length=20    \n",
    "    drop_width=2 \n",
    "    drop_color=(200,200,200) \n",
    "    ## a shade of gray    \n",
    "    rain_drops= generate_random_lines(imshape,slant,drop_length)        \n",
    "    for rain_drop in rain_drops:        \n",
    "        cv2.line(image,(rain_drop[0],rain_drop[1]),(rain_drop[0]+slant,rain_drop[1]+drop_length),drop_color,drop_width)    \n",
    "    image= cv2.blur(image,(7,7)) ## rainy view are blurry        \n",
    "    brightness_coefficient = 0.7 ## rainy days are usually shady     \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    image_HLS[:,:,1] = image_HLS[:,:,1]*brightness_coefficient ## scale pixel values down for channel 1(Lightness)    \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def generate_shadow_coordinates(imshape, no_of_shadows=1):    \n",
    "    vertices_list=[]    \n",
    "    for index in range(no_of_shadows):        \n",
    "        vertex=[]        \n",
    "        for dimensions in range(np.random.randint(3,15)): ## Dimensionality of the shadow polygon            \n",
    "            vertex.append(( imshape[1]*np.random.uniform(),imshape[0]//3+imshape[0]*np.random.uniform()))        \n",
    "            vertices = np.array([vertex], dtype=np.int32) ## single shadow vertices         \n",
    "            vertices_list.append(vertices)\n",
    "    return vertices_list ## List of shadow vertices\n",
    "\n",
    "\n",
    "def add_shadow(image,no_of_shadows=1):    \n",
    "    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS    \n",
    "    mask = np.zeros_like(image)     \n",
    "    imshape = image.shape    \n",
    "    vertices_list= generate_shadow_coordinates(imshape, no_of_shadows) #3 getting list of shadow vertices    \n",
    "    for vertices in vertices_list:         \n",
    "        cv2.fillPoly(mask, vertices, 255) ## adding all shadow polygons on empty mask, single 255 denotes only red channel        \n",
    "    image_HLS[:,:,1][mask[:,:,0]==255] = image_HLS[:,:,1][mask[:,:,0]==255]*7   ## if red channel is hot, image's \"Lightness\" channel's brightness is lowered     \n",
    "    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255 \n",
    "    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB    \n",
    "    return image_RGB\n",
    "\n",
    "def augmentData(original_image):\n",
    "    \n",
    "    blured_image = cv2.GaussianBlur(original_image,(15,15),0)\n",
    "    image_with_random_noise = np.uint8(util.random_noise(original_image)*255)\n",
    "    image_with_random_noise[image_with_random_noise > 255]  = 255\n",
    "    image_with_random_noise[image_with_random_noise < 0]  = 0\n",
    "    image_with_random_noise = np.uint8(image_with_random_noise)\n",
    "    v_min, v_max = np.percentile(original_image, (0.2, 99.8))\n",
    "    better_contrast = exposure.rescale_intensity(original_image, in_range=(v_min, v_max))\n",
    "    bright_image = add_brightness(original_image)\n",
    "    rainy_image = add_rain(original_image)\n",
    "    shadowed_image = add_shadow(original_image,no_of_shadows=1)\n",
    "    \n",
    "    return [original_image, blured_image, image_with_random_noise, bright_image, rainy_image]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the annotation list and filter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_annotations = pickle.load(open(path_to_tl_detection + '/annotations/' + model_name + '.p', 'rb'))\n",
    "filtered_annotations = []\n",
    "for i in range(0,len(read_annotations)):\n",
    "    filtered_annotations.append(filter_annotations(read_annotations[i], str(PATH_TO_TEST_IMAGES_DIR),show_image=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_list(probability, choice, alternative_choice):\n",
    "    if (random() <= probability):\n",
    "        return choice\n",
    "    else:\n",
    "        return alternative_choice\n",
    "\n",
    "def make_cvs_file(annotations, output_path, test_partition = 0.15):\n",
    "    annotation_lists = {}\n",
    "    annotation_lists[\"train\"] = []\n",
    "    annotation_lists[\"test\"] = []\n",
    "    counter = 0\n",
    "    for i in range(len(annotations)):\n",
    "        counter += 1\n",
    "        fname = str(counter).zfill(5)\n",
    "        \n",
    "        image = plt.imread(annotations[i][\"file_name\"])\n",
    "        \n",
    "        width = image.shape[1]\n",
    "        height = image.shape[0]\n",
    "        class_name = annotations[i][\"color\"]\n",
    "        boxes = annotations[i][\"boxes\"]\n",
    "        flipped_boxes = annotations[i][\"flipped_boxes\"]\n",
    "        \n",
    "        if (len(boxes) > 0):\n",
    "            images = augmentData(image)\n",
    "            \n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_rigal\" + \".jpg\"\n",
    "            plt.imsave(filename, images[0], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_blure\" + \".jpg\"\n",
    "            plt.imsave(filename, images[1], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_noise\" + \".jpg\"\n",
    "            plt.imsave(filename, images[2], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_brght\" + \".jpg\"\n",
    "            plt.imsave(filename, images[3], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_o_rainy\" + \".jpg\"\n",
    "            plt.imsave(filename, images[4], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "\n",
    "            images = augmentData(cv2.flip(image, 1))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_lippd\" + \".jpg\"\n",
    "            plt.imsave(filename, images[0], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_blure\" + \".jpg\"\n",
    "            plt.imsave(filename, images[1], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_noise\" + \".jpg\"\n",
    "            plt.imsave(filename, images[2], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_brght\" + \".jpg\"\n",
    "            plt.imsave(filename, images[3], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "            selection = select_list(test_partition, \"test\", \"train\")\n",
    "            filename = output_path + \"/images/\" + selection + \"/\" + fname + \"_f_rainy\" + \".jpg\"\n",
    "            plt.imsave(filename, images[4], format=\"jpg\")\n",
    "            for box in boxes:\n",
    "                annotation_lists[selection].append((filename, width, height, class_name, \\\n",
    "                                          box[0][0], box[0][1], box[1][0], box[1][1]))\n",
    "\n",
    "    \n",
    "    column_name = ['filename', 'width', 'height',\n",
    "                'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "    training_data_frame = pd.DataFrame(annotation_lists[\"train\"], columns=column_name)\n",
    "    training_data_frame.to_csv(output_path + \"/annotations/train_annotation_list.csv\")\n",
    "    test_data_frame = pd.DataFrame(annotation_lists[\"test\"], columns=column_name)\n",
    "    test_data_frame.to_csv(output_path + \"/annotations/test_annotation_list.csv\")\n",
    "    return test_data_frame, training_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, training_df = make_cvs_file(filtered_annotations, path_to_tl_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "nn=3\n",
    "NN=1\n",
    "for i in range(nn):\n",
    "    images = augmentData(plt.imread(filtered_annotations[NN+i][\"file_name\"]))\n",
    "    fig = plt.figure(figsize=(3*len(images),3))\n",
    "    for j in range(len(images)):\n",
    "        ax = plt.subplot(1,len(images),j+1)\n",
    "        ax.imshow(images[j])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(filtered_annotations[i][\"color\"], fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecords: /home/said/GIT/CarND-Capstone/tl_detection_pipeline/./annotations/train.record\n",
      "Successfully created the TFRecords: /home/said/GIT/CarND-Capstone/tl_detection_pipeline/./annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python scripts/generate_tfrecord_2.py --label=no_label --csv_input=./annotations/train_annotation_list.csv  --output_path=./annotations/train.record\n",
    "python scripts/generate_tfrecord_2.py --label=no_label --csv_input=./annotations/test_annotation_list.csv  --output_path=./annotations/test.record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/said/ML_tools/tensorflow37/models/research/object_detection/model_main.py\", line 26, in <module>\r\n",
      "    from object_detection import model_lib\r\n",
      "  File \"/home/said/ML_tools/tensorflow37/models/research/object_detection/model_lib.py\", line 27, in <module>\r\n",
      "    from object_detection import eval_util\r\n",
      "  File \"/home/said/ML_tools/tensorflow37/models/research/object_detection/eval_util.py\", line 40, in <module>\r\n",
      "    slim = tf.contrib.slim\r\n",
      "AttributeError: module 'tensorflow' has no attribute 'contrib'\r\n"
     ]
    }
   ],
   "source": [
    "# Here, just a test to see if the original script works as expected\n",
    "\n",
    "!python ~/ML_tools/tensorflow37/models/research/object_detection/model_main.py --pipeline_config_path=./training/ssd_inception_v2_coco.config --model_dir=./training/ssd_inception_v2_coco.config \\\n",
    "    --num_train_steps=500 --sample_1_of_n_eval_examples=1 --alsologtostderr\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "object_detection_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1LNYL6Zsn9Xlil2CVNOTsgDZQSBKeOjCh",
     "timestamp": 1566498233247
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566488313397
    },
    {
     "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/r2/tutorials/generative/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566145894046
    },
    {
     "file_id": "1nBPoWynOV0auSIy40eQcBIk9C6YRSkI8",
     "timestamp": 1566145841085
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556295408037
    },
    {
     "file_id": "1layerger-51XwWOwYMY_5zHaCavCeQkO",
     "timestamp": 1556214267924
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556207836484
    },
    {
     "file_id": "1w6mqQiNV3liPIX70NOgitOlDF1_4sRMw",
     "timestamp": 1556154824101
    },
    {
     "file_id": "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb",
     "timestamp": 1556150293326
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
